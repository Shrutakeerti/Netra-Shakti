import os
import chromadb
from langchain_ollama.llms import OllamaLLM
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader

# ================================
# üîπ Ensure ChromaDB Directory Exists
# ================================
CHROMA_DB_PATH = "./chroma_db"
if not os.path.exists(CHROMA_DB_PATH):
    os.makedirs(CHROMA_DB_PATH)

# ================================
# üîπ Load Ollama DeepSeek Model
# ================================
llm = OllamaLLM(model="deepseek-r1:1.5b")  # ‚úÖ Ensure model is correctly specified

# ================================
# üîπ Load Embedding Model
# ================================
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ================================
# üîπ Initialize ChromaDB
# ================================
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
collection = chroma_client.get_or_create_collection("eye_health")  # ‚úÖ Collection for storing knowledge

# ================================
# üîπ Load PDFs and Store Embeddings
# ================================
def load_pdfs(pdf_folder="data"):
    """ Loads PDFs, splits text into chunks, and stores embeddings in ChromaDB. Runs only once. """
    if os.path.exists("pdfs_loaded.flag"):
        print("‚úî PDFs are already loaded, skipping...")
        return

    print("üîÑ Loading PDFs into ChromaDB...")
    
    for pdf_file in os.listdir(pdf_folder):
        if pdf_file.endswith(".pdf"):
            pdf_path = os.path.join(pdf_folder, pdf_file)
            try:
                pdf_loader = PyPDFLoader(pdf_path)
                docs = pdf_loader.load()
            except ModuleNotFoundError:
                print("‚ùå ERROR: Install 'pypdf' ‚Üí `pip install pypdf`")
                continue
            except Exception as e:
                print(f"‚ùå Error loading PDF {pdf_file}: {e}")
                continue

            if docs:
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
                chunks = text_splitter.split_documents(docs)

                for i, chunk in enumerate(chunks):
                    text = chunk.page_content
                    try:
                        embedding = embedding_model.embed_query(text)
                        collection.add(
                            documents=[text],
                            embeddings=[embedding],
                            ids=[f"{pdf_file}_{i}"]
                        )
                    except Exception as e:
                        print(f"‚ùå Error storing chunk {i} of {pdf_file}: {e}")

    # ‚úÖ Create a flag file after successful processing
    with open("pdfs_loaded.flag", "w") as f:
        f.write("PDFs loaded")
    
    print("‚úÖ PDF Processing & Embedding Done!")

# ================================
# üîπ Query Knowledge Base Function
# ================================
# üîπ Query Knowledge Base Function
# ================================
def query_knowledge_base(user_query):
    """
    Retrieves relevant context from ChromaDB and generates a response using DeepSeek-R1-1.5B via Ollama.
    """
    print(f"\nüîç Query: {user_query}")

    try:
        # Retrieve top matching text from ChromaDB
        results = collection.query(query_embeddings=[embedding_model.embed_query(user_query)], n_results=3)
        
        # Debugging: Print raw ChromaDB results
        print("üìå ChromaDB Results:", results)

        retrieved_docs = results.get("documents", [])
        if not retrieved_docs or not retrieved_docs[0]:
            return "No relevant information found in the knowledge base."

        # Combine retrieved text chunks
        context = "\n".join(retrieved_docs[0])

        # Debugging: Print retrieved context
        print("üìå Retrieved Context:\n", context)

        # Format prompt for DeepSeek-R1-1.5B
        prompt = f"Answer the following based on the given context:\n\nContext: {context}\n\nQuestion: {user_query}\n\nAnswer:"

        # ‚úÖ Generate response using Ollama
        response = llm.invoke(prompt)

        return response
    except Exception as e:
        print(f"‚ùå Chatbot Backend Error: {e}")
        return "‚ùå Error: Backend Issue"

# ================================
# üîπ Example Usage
# ================================
if __name__ == "__main__":
    print("\nüîπ Loading PDFs & Creating Embeddings...")
    load_pdfs()  # Load PDFs only if not already loaded

    user_query = "What are the causes of Cataract?"
    response = query_knowledge_base(user_query)
    print("\nChatbot:", response)
